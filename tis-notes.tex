\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{inputenc}
\usepackage[a4paper, total={150mm,250mm}]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[dvipsnames, table]{xcolor}
\usepackage{subcaption}
\usepackage{fancybox, graphicx}
\usepackage{tikz}
\usepackage{array}
\usepackage{ulem}
\usepackage{enumitem}
\usetikzlibrary{shadows}
\usepackage{listings}
\usepackage{bm}
\usepackage{lmodern,textcomp}
\usepackage{listings}
%\usepackage[english]{babel}

\newcommand{\nline}{\\~\\}
\newcommand{\myparagraph}[1]{\paragraph{\normalsize{\textcolor{gray}{\uppercase{\textbf{#1}}}} }\mbox{} \vspace{0.5em}\\}
\usepackage{tikz}
\hypersetup{
    colorlinks=true, %set true if you want colored links
	linkcolor=black,
    linktoc=all,     %set to all if you want both sections and subsections linked 
    urlcolor=blue
}
\title{{\Huge\textbf{TIS  Notes}}

\vspace{1em}
\normalsize{\textbf{Author}: \href{https://www.linkedin.com/in/simone-staffa-8b3b79158/}{Simone Staffa} \linebreak}

\vspace{1em}
\large \textcolor{red}{\textbf{Disclaimer!!} \\
\normalsize{This document is not intended to be a source of truth to prepare the exam. The materials included here are taken from the official course slides, I'm not responsible for wrong answers or errors reported here (even though I've done my best to provide valid material that regards all the topics that were spoken in the course). \\
This notes are an extension of this document: \href{https://www.andreadd.it/appunti/polimi/ingegneria/corsi/ing_informatica/Magistrale/TABA/technologies_for_information_systems/viewer.html?file=appunti/appunti_riassuntivi_del_corso.pdf}{Appunti Riassuntivi del Corso}
 }} \\
\vspace{30em}

\small{
Released with Beerware License, Rev. 42 (https://spdx.org/licenses/Beerware.html) \linebreak
“As long as you retain this notice you can do whatever you want with this stuff. If we meet some day, and you think this stuff is worth it, you can buy me a beer in return”}
}
\begin{document}
\maketitle
\clearpage
\tableofcontents
\clearpage
\section{Data Integration}
\subsection{Introduction}
\textbf{Data Integration is the problem of combining data coming from different data sources}, providing the user with a unified vision of the data, detecting correspondences between similar concepts that come from different sources, and solving possible conflicts. 
\nline
The aim of Data Integration is to set up a system where it is possible to query different data sources as if they were a unique one (through a global schema).
\subsubsection{Interoperability}
Data integration is needed because of a need for interoperability among SW applications, services and information managed by different organizations:
\begin{itemize}
	\item find information and processing tools, when they are needed, independently of physical location
	\item understand and employ the discovered information and tools, no matter what platform supports them, whether local or remote
	\item evolve a processing environment for commercial use without being constrained to a single vendor’s offerings.
\end{itemize}
\subsubsection{4 Vs of Big Data in data integration}
\begin{itemize}
	\item \textbf{Volume} (data at scale): not only can each data source contain a \uline{huge volume of data}, but also the number of data sources has grown to be in the millions.
	\item \textbf{Velocity} (data in motion): as a direct consequence of the \uline{rate} at which data is being collected and continuously made available, many of the data sources are very dynamic.
	\item \textbf{Variety} (data in many form): data sources (even in the same domain) are extremely \textit{heterogeneous} both at:
	\begin{itemize}
		\item the \uline{schema level}, regarding how they structure their data, 
		\item and at the \uline{instance level}, regarding how they describe the same real world entity
\end{itemize}
exhibiting considerable variety even for substantially similar entities
	\item \textbf{Veracity} (data uncertainty): data sources (even in the same domain) are of widely differing \uline{qualities}, with significant differences in the coverage, accuracy and timeliness of data provided. This is consistent with the observation that "1 in 3 business leaders do not trust the information they use to make decisions".
\end{itemize}
In particular we are interested in:
\begin{itemize}
	\item \textbf{The Variety Dimension}: people and enterprises need to integrate data and the systems that handle those data (relational DBMSs and their extensions containing heterogeneous content)
	\item \textbf{The Veracity Dimension}: data quality is the most general and used term, and represents a number of quality aspects besides veracity:
	\begin{itemize}
		\item Completeness (essential fields are present)
		\item Validity (soundness)
		\item Consistency (no contradiction)
		\item Timeliness (up-to-date)
		\item Accuracy (registered in an accurate way)
	\end{itemize}
\end{itemize}
\subsubsection{Heterogeneity}
Heterogeneity derives from various forms of AUTONOMY.
\begin{itemize}
	\item \textbf{Design} (representation) autonomy: design a dataset in different ways w.r.t. to others
	\item \textbf{Communication} (querying) autonomy: way in which we query the data
	\item \textbf{Execution} (algorithmic) autonomy: each DBMS has its way of extracting data
\end{itemize}
There is a \textbf{need for interoperability} among software applications, services and information (databases, and others) managed by different organizations that need to reuse legacy applications, existing data repositories (e.g., deep web), and reconcile the different points of view adopted by the various players using the information.
\myparagraph{VARIETY (heterogeneity)}
Variety among several data collections to be used together:
\begin{itemize}
	\item \textbf{Different platforms}: technological heterogeneity
	\item \textbf{Different data models at the participating DBMS}: model heterogeneity
	\item \textbf{Different query languages}: language heterogeneity
	\item \textbf{Different data schemas and different conceptual representations in DBs previously developed}: schema heterogeneity
	\item \textbf{Different values for the same info} (due to errors or to different knowledge): instance (semantic) heterogeneity
\end{itemize}
\myparagraph{VERACITY}
Main Data Quality dimensions:
\begin{itemize}
	\item Completeness
	\item Validity
	\item Consistency
	\item Timeliness
	\item Accuracy
\end{itemize}
\subsection{The Steps of Data Integration}
\begin{enumerate}
	\item \textbf{Schema reconciliation}: mapping the data structure
	\item \textbf{Record linkage (aka Entity resolution)}: data matching based on the same content, that is understanding which record corresponds to the others in different data sources
	\item \textbf{Data fusion}: reconciliation of non-identical content, that is deciding which of the two linked record to maintain
\end{enumerate}
There are \uline{two relevant ways of integrating database systems}:
\begin{enumerate}
	\item Use a \textbf{materialized data base} (data are merged in a new database) $\rightarrow$ through the use of Extract-Transform-Load systems
	\begin{itemize}
		\item Data warehouses are materialized integrated data sources
	\end{itemize}
	\item Use a \textbf{virtual non-materialized data base} (data remain at sources) $\rightarrow$ produce an interface to expose the unified view of data sources
	\begin{itemize}
		\item Enterprise Information Integration Systems (common front-end to the various data sources)
		\item Data Exchange (source-to-target)
	\end{itemize}
\end{enumerate}
\begin{figure}[htp]
\centering
\includegraphics[width=.4\textwidth]{images/materialized-integration}\hfill
\includegraphics[width=.4\textwidth]{images/virtual-integration}\hfill
\caption{\textbf{Materialized Integration} (left): a physical view aggregatin different sources over one single structure. Every X days we need to synchronize/update the materialized DB, that is a different physical DB in the system (offline approach). \\
\textbf{Virtual Integration} (right): virtual view over different sources providing one single structure. No need to install a new physical DB. The view is always up to date (online approach).}
\end{figure}
\raggedright
\myparagraph{Materialized Integration}
Large common stores came to be known as \textbf{warehouses}, and the software to access, scrape, transform, and load data into warehouses, became known as \uline{extract, transform, and load (ETL)} systems. \\ In a dynamic environment, one must perform ETL periodically (e.g., once a day or once a week), thereby building up a history of the enterprise. \textbf{The main purpose of a data warehouse is to allow systematic or ad-hoc data analysis and mining.} Indeed, warehouses are not appropriate when need to integrate the operational systems (keeping data up-to-date).
\myparagraph{Virtual Integration}
The \textbf{virtual integration approach leaves the information requested in the local sources.} The virtual approach will always return a fresh answer to the query. The query posted to the global schema is reformulated into the formats of the local information system. The information retrieved needs to be combined to answer the query.
\myparagraph{Rationale}
The conventional wisdom is to use data warehousing and ETL products to perform data integration. However, there is a serious flaw in one aspect of this wisdom. \nline
Suppose one wants to integrate current (operational) data rather than historical information. Consider, for example, an e-commerce web site which wishes to sell hotel rooms over the web. The actual inventory of available hotel rooms exists in 100 or so information systems since Hilton, Hyatt, and Marriot all run their own reservation systems.\\ Applying ETL and warehouses to this problem will create a copy of hotel availability data, which is quickly out of date. \uline{If a website sells a hotel room, based on this data, it has no way of guaranteeing delivery of the room, because somebody else may have sold the room in the meantime.}
\subsubsection{Design Steps for Data Integration}
Data integration problems arise even in the simplest situation, that is with a unique, centralized database. \\
\textbf{UNIQUE DB}: each datum, whatever application uses it, must only appear once. This eliminates useless redundancies which would cause inconsistencies and useless memory occupation. \nline
The simplest case of non-centralized database.\\
\textbf{DISTRIBUTED DB}: non-centralized DB; distribution design decisions are on fragmentation (horizontal or vertical), allocation, replication. \nline
We study data integration for:
\begin{itemize}
	\item \textbf{Federated DB}: organizations that have been merged together
	\begin{itemize}
		\item \uline{Homogeneous data}: same data model
		\item \uline{Heterogeneous data}: different data models (structured, unstructured or semi-structured)
	\end{itemize}
	\item \textbf{Transient, initially unknown} data sources
\end{itemize}
Designing data integration in the Multidatabase (with a unique data model):
\begin{enumerate}
	\item Source schema identification (when present)
	\item Source schema reverse engineering (data source conceptual schemata)
	\item Conceptual schemata integration and restructuring (related concept identification, conflict analysis and resolution, conceptual schema integration)
	\item Conceptual to logical translation of the obtained global schema
	\item Mapping between the global logical schema and the single schemata (logical view definition)
	\item After integration: query answering through data views
\end{enumerate}

\subsection{Conflict Analysis}
\subsubsection{Conflict Types}
\begin{itemize}
	\item \textbf{Name conflicts} (homonymies or synonymies e.g., customer vs client)
	\item \textbf{Type conflicts}: in a single attribute (e.g the attribute gender: Male/Female, M/F, 0/1) or in an entity type (different abstractions of the same real world concept produce different sets of attributes).
	\item \textbf{Data semantics}: different currencies (euros, dollars, etc), different measure systems (kilos vs pounds, centigrades vs Fahrenheit), different granularities (grams, kilos, etc).
	\item \textbf{Structure conflicts}: e.g. the entity Person is extended by entities Male and Female, or entity Person has an attribute “gender” (a concept is an attribute in a source and an entity in another one).
	\item \textbf{Cardinality conflicts} (e.g. a movie may have just one director in a source, while another source allows more directors).
	\item \textbf{Key conflicts}: two same entities have two different keys (e.g. in a source the person is identified by the SSN and in another source it is identified through the email address).
\end{itemize}
\subsection{Mapping between the Global Logical Schema and the Single Source Schemata (logical view definition)}
There are two approaches for schema mapping:
\begin{itemize}
	\item \textbf{GAV} (Global As View): the global schema is derived from the integration process of the data source schemata, thus the global schema is expressed in terms of the data source schemata. This approach is appropriate for stable data sources, it is in fact difficult to extend with a new data source. Mapping quality depends on how well we have compiled the sources into the global schema through the mapping. Whenever a source changes or a new one is added, the global schema needs to be reconsidered.
	\item \textbf{LAV} (Local As View): the global schema is designed independently of the data source schemata. The relationship (mapping) between sources and global schema is obtained by defining each data source as a view over the global schema. This approach is appropriate if the global schema is stable, and it favors extensibility. By the way, query processing is much more complex. Mapping quality depends on how well we have characterized the sources. 
\end{itemize}
A third approach is the union of the previous two:
\begin{itemize}
	\item \textbf{GLAV} ( Global and Local As View): the relationship (mapping) between sources and global schema is obtained by defining a set of views, some over the global schema and some over the data sources.
\end{itemize}
The most useful integration operators to write relational GAV views are:
\begin{itemize}
	\item \textbf{union}
	\item \textbf{outerunion}: used with different schemas, putting null all the information we don’t have. There are all the attributes of the tow sources (without repetitions)
	\item \textbf{outerjoin}: it doesn’t admit different values for 1 attribute
	\item \textbf{generalization}: we keep only common attributes
\end{itemize}
A mapping defined over some data source is \textbf{SOUND} when it provides a subset of the data that is available in the data source that corresponds to the definition. \\
A mapping is \textbf{COMPLETE} if it provides a superset of the available data in the data source that corresponds to the definition. \\
A mapping is \textbf{EXACT} if it provides all and only data corresponding to the definition: it is both sound and complete. \\
With the GAV approach, the mapping can be exact or only sound. \\
With the LAV approach, the mapping can be exact or only complete, due to the incompleteness of one or more sources (they do not cover the data “expected” from the global schema, which has been defined independently of the source contents).
\subsection{Inconsistencies in the Data}
At query processing time, when a real world object is represented by instances in different databases, they may have different values.
\begin{itemize}
	\item \textbf{Record Linkage (aka Entity Resolution)}: finding the info that refer to same real-world entities
	\item \textbf{Data Fusion}: once recognized that two items refer to the same entity, how do we reconcile inconsistent information?
\end{itemize}
\subsubsection{Record Linkage (aka Entity Resolution)}
Whatever the data model, we have to recognize when two datasets contain the same information. \\ 
Considering the case of a relational database (the most common), several techniques can be used:
\begin{itemize}
	\item \textbf{String matching}: concatenate multiple columns into a string, then compare the two strings. Not suggested!
	\item \textbf{Tuple (or structured-data) matching}: compare records field by field. It is much easier to spot similarities, since we can also associate different meanings to the columns.
	\item \textbf{String similarity and similarity measures}: given two sets of strings, find all pairs from the two sets that refer to the same real-world entity. Each of these pairs is called a match.  \\
	\uline{Types of similarity measures:}
	\begin{itemize}
		\item Sequence-based: 
		\begin{itemize}
			\item \textit{edit-distance}: based on the minimal number of operations that are needed to transform string a to string b
\end{itemize}
		\pagebreak
		\item Set-based:
		\begin{itemize}
			\item \textit{Jaccard}: divide the strings into tokens, and compute the measure on the two sets of tokens (intersection divided by union of tokens)
		\end{itemize}
		\item Phonetic:
		\begin{itemize}
			\item \textit{Soundex}: calculates a four-character code from a word based on the pronunciation and considers two words as similar if their codes are equal. Similar sounding letters are assigned the same soundex code.
		\end{itemize}
	\end{itemize}
	\item \textbf{Record Matching:}
	\begin{itemize}
		\item \textit{Rule-based matching}: manually written rules that specify when two tuples match (e.g., two tuples refer to the same person if they have the same SSN)
		\item \textit{Learning Matching Rules}: 
		\begin{itemize}
			\item Supervised: learn how to match from training data, then apply it to match new tuple pairs (requires a lot of training data)
			\item Unsupervised: clusterizing records based on similar values
		\end{itemize}
		\item \textit{Probabilistic Matching}: model the matching domain using a probability distribution. Provides a principled framework that can naturally incorporate a variety of domain knowledge. By the way, it is computationally expensive.
	\end{itemize}
\end{itemize}
\subsubsection{Data Fusion}
Once you have understood that some data clearly represent the same entity, you still have to cope with the problem of what to do when other parts of the info do not match. \nline
Inconsistency may depend on different reasons:
\begin{itemize}
	\item One (or both) the sources are incorrect
	\item Each source has a correct but partial view, e.g., databases from different workplaces (salary is the sum of the two)
	\item Often the correct value may be obtained as a function of the original ones
\end{itemize}
\subsection{Data Model Heterogeneity}
\textbf{Wrappers} are elements that convert queries into queries/commands which are understandable for the specific data source. They convert query results from the source format to a format which is understandable for the application.
 \nline
 Design steps for data integration (data model heterogeneity):
\begin{enumerate}
	\item Reverse engineering (production of the conceptual schema)
	\item Conceptual schemata integration
	\item Choice of the target logical data model and translation of the global conceptual schema 
	\item Definition of the language translation (wrapping)
	\item Definition of the data views.
\end{enumerate}
\pagebreak
\section{Semistructured Data Integration}
For semistructured data there is some form of structure, but it is not as prescriptive, regular and complete as in traditional DBMSs.
\begin{itemize}
	\item text
	\item trees
	\item graphs
\end{itemize}
They are all different and do no lend themselves to easy integration. \nline
We would like to:
\begin{itemize}
	\item integrate
	\item query
	\item compare
\end{itemize}
data with different structures also with semistructured data, as if they were all structured. An overall data representation should be progressively built, as we discover and explore new information sources. \nline
\subsection{Mediators}
A \textbf{mediator} has the same purpose as the integration systems. Mediators are interfaces specialized in a certain domain which stand between application and wrappers. They accept queries written in the application’s language, decompose them and send them to each specific wrapper. They also send the responses back to the application, providing a unified vision of data.
\\
The term \uline{mediation} includes:
\begin{itemize}
	\item the \textbf{processing} needed to make the interface work
	\item the \textbf{knowledge structures} that drive the transformations needed to transform data to information
	\item any \textbf{intermediate storage} that is needed
\end{itemize}
\subsubsection{Tsimmis}
\textbf{TSIMMIS} is the first system based on the mediator/wrapper paradigm, proposed in the 90’s at Stanford. \\ 
In this system:
\begin{itemize}
	\item unique, graph-based data model
	\item data model managed by the mediator
	\item wrappers for the model-to-model translations
	\item query posed to the mediator
	\item mediator knows the semantics of the application domain
\end{itemize}
Each mediator is specialized into a certain domain. Thus, each mediator must know domain metadata which convey the data semantics. If data source changes a little, the wrapper has to be modified $\rightarrow$ automatic wrapper generation. \\
In TSIMMIS, queries are posed to the mediators in an object-oriented language \textbf{LOREL} (Lightweight Object REpository Language). The data model adopted is OEM (Object Exchange Model), a graph-based and self-descriptive model, and it represents directly data with no schema at all.
\subsection{Ontologies}
\subsubsection{Problem in information extraction from HTML docs}
\begin{itemize}
	\item Web sites change very frequently
	\item a layout change may affect the extraction rules
	\item human-based maintenance of an ad-hoc wrapper is very expensive
$\rightarrow$ automatic wrapper generation is better, especially when many pages share the same structure, when pages are dynamically generated from a DB.
\end{itemize}
We can only use automatic wrapper generation when pages are regular to some extent.
\subsubsection{Ontologies, a way to solve the problem of automatic semantic matching}
An \textbf{ontology}:
\begin{itemize}
	\item is a formal specification of a conceptualization of a shared knowledge domain. 
	\item is a formal and shared definition of a vocabulary of terms and their inter-relationships. 
	\item is a controlled vocabulary that describes objects and the relationships between them in a formal way. 
	\item it has a grammar for using the terms to express something meaningful within a specific domain of interest
\end{itemize} 
Predefined relations: 
\begin{itemize}
	\item synonimy
	\item homonymy
	\item hyponymy
\end{itemize} 
N.B. An ER diagram, a class diagram, any conceptual schema is an ontology. \nline
\textbf{Formal Specification} allows for use of a common vocabulary for \uline{automatic knowledge sharing}. \\
\textbf{Shared}: an ontology captures knowledge which is common. \\
\textbf{Conceptualization}: gives a unique meaning to the terms that define the knowledge about a given domain. \nline

\textbf{Ontology Types}:
\begin{itemize}
	\item \textbf{Taxonomic ontologies}: definition of concepts through terms, their hierarchical organization, and additional (predefined) relationships
	\item \textbf{Descriptive ontologies}: definition of concepts for alignment of existing data structures or to design new specialized ontologies (domain ontologies). Descriptive ontologies require rich models to enable representations close to human perception.
\end{itemize}

An ontology consists of:
\begin{itemize}
	\item \textbf{concepts}: generic concepts (express general world categories), specific concepts (describe a particular application domain, domain ontologies)
	\item \textbf{concept definition} (in natural language or via a formal language)
	\item \textbf{relationships between concepts}: taxonomies, user-defined associations, synonymies,...
\end{itemize}
An ontology is composed by:
\begin{itemize}
	\item a \textbf{T-Box}: contains all the concept and role definitions, and also contain all the axioms of our logical theory (e.g., "A father is a Man with a Child").
	\item an \textbf{A-Box}: contains all the basic assertions of the logical theory (e.g., "Tom is a father" is represented as Father(Tom)).
\end{itemize}
\pagebreak
\subsection{Semantic Web}
\textbf{Semantic Web} is a vision for the future of the Web in which information is given explicit meaning, making it easier for machines to automatically process and integrate information available on the Web. It is built on XML's ability to define customized tagging schemes and RDF's flexible approach to representing data.
 \nline
\textbf{Linked Data} consists of connecting datasets across the Web. It describes a method of publishing structured data so that it can be interlinked and become more useful. It builds upon standard Web technologies such as HTTP, RDF and URIs, but extends them to share information in a way that can be read automatically by computers, enabling data from different sources to be connectd and queried. \nline
\uline{Open data} is data uploaded to the Web and accessible to all. \\
\uline{Linked Open data} extend the web with a data commons by publishing various open datasets as RDF on the Web and by setting RDF links among them.
\nline
\subsubsection{RDF and OWL}
OWL is the first level above RDF. It is an ontology language that can formally describe the meaning of terminology used in Web documents, going beyond the basic semantics of RDF schema.
\begin{itemize}
	\item \textbf{XML}: provides a syntax for structured documents, but imposes no semantic constraints on the meaning of these documents.
	\item \textbf{XML Schema}: is a language for restricting the structure of XML documents and also extends XML with data types.
	\item \textbf{RDF}: is a data model for objects (resources) and relations between them, provides a simple semantic for this data model, and can be represented in an XML syntax.
	\item \textbf{RDF Schema}: is a vocabulary for describing properties and classes of RDF resources
\end{itemize}
\textbf{OWL} adds more vocabulary for describing properties and classes. The OWL (Web Ontology Language) is designed for use by applications that need to process the content of information instead of just presenting information to humans. It facilitates greater machine interpretability of Web content than that supported by XML, RDF and RDF Schema by providing additional vocabulary along with a formal semantics.\\
	It has three increasingly-expressive sublanguages:
	\begin{itemize}
		\item \uline{OWL Lite}: supports users primarily needing a classification hierarchy and simple constraints. It has a lower formal complexity than OWL DL.
		\item \uline{OWL DL}: supports users who want maximum expressiveness while all conclusions are guaranteed to be computed (computational completeness) and all computations will finish in a finite time (decidability).
		\item \uline{OWL Full}: meant for users who want maximum expressiveness and the syntactic freedom of RDF: no computational guarantees.
	\end{itemize}
\subsection{Reasoning Services}
Services for the T-Box:
\begin{itemize}
	\item \textbf{Subsumption}: verifies if a concept C subsumes (is a subconcept of) another concept D
	\item \textbf{Consistency}: verifies that there exists at least one interpretation which satisfies the given Tbox
	\item \textbf{Local Satisfiability}: verifies for a given concept that there exists at least one interpretation in which it is true.
\end{itemize}
\pagebreak
Services for the A-Box:
\begin{itemize}
	\item \textbf{Consistency}: verifies that an A-Box is consistent w.r.t a given Tbox
	\item \textbf{Instance Checking}: verifies if a given individual belongs to a particular concept
	\item \textbf{Instance retrieval}: returns the extension of a given concept C, that is the set of individuals belonging to C.	
\end{itemize}
\subsection{Ontologies and Integration Problems}
How should we improve database conceptual models to fulfill ontology requirements?
\begin{itemize}
	\item Supporting defined concepts (views) and adding the necessary reasoning mechanisms
	\item Managing missing and incomplete information: explicit semantic differences between the two (referring to facts)
	\begin{itemize}
		\item Closed World Assumption $\rightarrow$ whatever is not in the database is FALSE
		\item Open World Assumption $\rightarrow$ if an information is missing it is possible that is TRUE or FALSE
	\end{itemize}
\end{itemize}
Problems:
\begin{itemize}
	\item \uline{Discovery of equivalent concepts (mapping)}: what does equivalent mean? (we look for some kind of similarity)
	\item \uline{Formal representation of these mappings}: how are these mapping represented?
	\item \uline{Reasoning of these mappings}: how do we use the mappings within our reasoning and query-answering process?
\end{itemize}
\textbf{Ontology matching} is the process of finding pairs of resources coming from different ontologies which can be considered equal in meaning. We need some kind of \textit{similarity measure}, this time taking into account also semantics (i.e., not only the structure of words). \\
As already seen, similarity is strictly related to \uline{distance}. 
Three main categories:
\begin{itemize}
	\item Metric (distance)-based measures
	\item Set-theoretic based measures
	\item Implicators based measures
\end{itemize}
\textbf{Ontology mapping} is the process of relating similar concepts or relations of two or more information sources using equivalence relations or order relations.
\nline
\uline{Reasons for ontology mismatches}:
\begin{itemize}
	\item \uline{Scope}: two classes seem to represent the same concept but do not have exactly the same instances
	\item \uline{Model coverage and granularity}: a mismatch in the part of the domain that is covered by the ontology, or the level of detail to which that domain is modeled (e.g., ontology for all the animals vs. ontology for birds)
	\item \uline{Paradigm}: different paradigms can be used to represent concepts such as time (e.g., temporal representations $\rightarrow$ continuous interval vs. discrete sets of time points
	\item \uline{Encoding}
	\item \uline{Concept description}: a distinction between two classes can be modeled using a qualifying attribute or by introducing a separate class, or the way in which is-a hierarchy is built.
	\item \uline{Homonyms and Synonyms}
\end{itemize}
\pagebreak
\subsubsection{How can ontologies support integration?}
\textbf{An ontology can be a schema integration support tool}. Ontologies are used to represent the semantics of schema elements (if the schema exists). \\
\textbf{An ontology can be used instead of a global schema}:
\begin{itemize}
	\item schema-level representation only in terms of ontologies
	\item ontology mapping, merging etc. instead of schema integration
	\item integrated ontology used as a schema for querying
\end{itemize}
Data source heterogeneity is solved by extracting the semantics in an ontological format. Data source ontologies are mapped to the Domain Ontology (Global Schema). \nline
\textbf{An ontology as a support for content interpretation and wrapping} (e.g., HTML pages). \\ \textbf{An ontology as a mediation support tool for content inconsistency detection and resolution} (record linkage and data fusion).
\nline
When we use ontologies to interact with databases we have to take care of:
\begin{itemize}
	\item Transformation of ontological query into the language of the datasource, and the other way round
	\item Different semantics (CWA vs. OWA)
	\item What has to be processed where (e.g., push of the relational operators to the relational engine)
\end{itemize}
\pagebreak
\section{New trends in Data Integration Research and Development}
\subsection{Uncertainty in Data Integration}
Databases are assumed to represent certain data (a tuple in the database is true). But real life is not as certain. Uncertain databases attempt to model uncertain data and to answer queries in an uncertain world. \\Moreover:
\begin{itemize}
	\item Data itself may be uncertain (e.g. extracted from an unreliable source);
	\item Mappings might be approximate
	\item Reconciliation is approximate
	\item Imprecise queries are approximate
\end{itemize}
Whatever the semantics of uncertainty (e.g, fuzzy or probabilistic..) an uncertain database describes a set of possible worlds. \\
Example: assign each tuple a probability. Then the probability of a possible world is the product of the probabilities for the tuples.
\subsubsection{Entity Resolution}
Entity Resolution (ER) is the problem of accurately identifying multiple, differing and possibly contradicting representations of unique real-world entities in data. \\
The causes of multiple representations of the same real-world entity are:
\begin{itemize}
	\item Noise (small errors)
	\item Inconsistencies (conflicting or different values for properties/relations of an object The record matching rules are used to detect duplicates in data.
\end{itemize}
A common approach is to use \textbf{record-matching rules}. Record-mathing rules are constraints that state: \textit{"if any two records are similar on properties P1,...Pn, then they refer to the same entity".} \nline
Problems of this approach:
\begin{itemize}
	\item it is difficult to generate automatically record-matching rules
	\item one-to-one record matching is too expensive (quadratic)
\end{itemize}
\subsubsection{Data Provenance}
Sometimes knowing where the data have come from and how they were produced is critical (e.g., an information extractor might be unreliable, or one data source is more authoritative than others). \\
\textbf{Provenance of a data} item records "where it comes from":
\begin{itemize}
	\item Who created it
	\item When it was created
	\item How it was created (as value in a database, as the result of a computation, coming from sensor, etc ...)
\end{itemize}
\uline{Two viewpoints on Provenance}:
\begin{itemize}
	\item \textbf{Provenance as annotations on data}: models provenance as a series of annotations describing how each data item was produced. These annotations can be associated with tuples or values
	\item \textbf{Provenance as a graph of data relationships}: models provenance as a graph, with tuples as vertices. Each possible direct derivation of a tuple from a set of source tuples is a edge-connecting the source and derived tuples.
\end{itemize}
\subsection{Building Large-Scale Structured Web Databases}
Develop methodologies for designing a fully integrated database coming from heterogeneous data sources.
\subsubsection{Lightweight Integration}
We may need to integrate data from multiple sources to answer a question asked once or twice. The integration must be done quickly and by people without technical expertise.
\\ Problems typical of lightweight data integration:
\begin{itemize}
	\item Locating relevant data sources
	\item Assessing source quality
	\item Helping the user understand the semantics
	\item Support the process of integration
\end{itemize}
\subsubsection{Mashup}
\textbf{Mashup} is a paradigm for lightweight integration. \\
It is an application that integrates two or more mashup components at any of the application layers (data, application logic, presentation layer) possibly putting them in communication with each other. \\
\uline{Key elements}:
\begin{itemize}
	\item \textbf{Mashup component}: is any piece of data, application logic and/or user interface that can be reused and that is accessible either locally or remotely (e.g., Craiglist and GoogleMaps)
	\item \textbf{Mashup logic}: is the internal logic of operation of a mashup; it specifies the invocation of components, the control flow, the data flow, the data transformations and the UI of the mashup.
\end{itemize}
Mashups introduce integration at the presentation layer and typically focus on non-mission-critical applications. 
\myparagraph{Benefits and Problems}
\uline{Benefits}:
\begin{itemize}
	\item Easy development of situational applications for power users
	\item Fast prototyping for developers
	\item Increased ROI for Service-Oriented-Application investments
\end{itemize}
\uline{Problems}: \\
Mashup development is not easy. Luckily, mashups typically work on the "surface".
\begin{itemize}
	\item Reuse of existing components
	\item Composition of the outputs of software systems
\end{itemize}
The work of developers can be facilitated by suitable abstractions, component technologies, development paradigms and enabling tools. 
\myparagraph{Types of Mashups}
\begin{itemize}
	\item \textbf{Data mashups}: retrieve data from different resources, process them and return an integrated result set. Data mashups are a Web-based, lightweight form of data integration, intended to solve different problems.
	\item \textbf{Logic mashups}: integrate functionality published by logic or data components. The output is a process that manages the components, in turn published as a logical component.
	\item \textbf{User interface mashups}: combine the component’s native UIs into an integrated UI. The output is a Web application the user can interact with. It is mostly client-side, generally short-lived.
	\item \textbf{Hybrid mashups}: span multiple layers of the application stack, bringing together different types of components inside one and a same application.
\end{itemize}
\subsubsection{Pay-as-you-go management}
In contrast the other kind of data integration, \textbf{pay-as-you-go systems} try to avoid the need for the initial setup phase that includes creating the mediated schema and source descriptions. The goal is to offer a system, that provides useful services on a collection of heterogeneous data with very little initial effort.
\subsection{Data Spaces}
A Database Management System (DBMS) is a generic repository for the storage and querying of structured data. Unfortunately in data management scenarios today it is rarely the case that all the data can be fit nicely into a conventional relational DBMS, or into any other single data model or system. \\
One solution is \textbf{Dataspaces}: they are not a data integration approach, rather, they are more of a data coexistence approach. The goal of data space support is to provide base functionality over all data sources, regardless of how integrated they are.
The required techniques for actually deploying Dataspaces are:
\begin{itemize}
	\item Schema mapping
	\item Uniform search over multiple types of data
	\item Combining structured, semi-structured and unstructured data
	\item Approximate query processing
	\item Managing and querying uncertain data and its lineage
	\item Stream and sensor data management and processing.
\end{itemize}
A Dataspace is modeled as a set of \textbf{Participants} and \textbf{Relationships}. The \uline{participants in a data space are the individual data sources} (relational databases, XML repositories, text databases..). Some participants may support expressive query languages, while others are opaque and offer only limited interfaces for posing queries. A dataspace should be able to model any kind of relationship between two or more participants. \uline{Relationships may be stored as query transformations}, dependency graphs, or sometimes even textual descriptions. The catalog contains information about all the participants in the data space and relationships among them.
Users should be able to query any data item regardless of its format or data model.
\begin{figure}[htp]
\centering
\includegraphics[width=.5\textwidth]{images/data-spaces}\hfill
\includegraphics[width=.5\textwidth]{images/data-spaces-vs-dw}\hfill
\end{figure}
\pagebreak
\section{Data Analysis and Exploration}
\subsection{Analysis of Data}
\textbf{Data Analysis} is a process of inspecting, cleaning, transforming and modeling data with the goal of highlighting useful information, suggesting conclusions, and supporting decision making.
\nline
\textbf{Data exploration} is a preliminary exploration of the data to better understand its characteristics.
\nline
\textbf{Data mining} is a particular data analysis technique that focuses on modeling and knowledge discovery for predictive rather than purely descriptive purposes.

\textbf{Machine Learning} is a field of study that gives computers the ability to learn without being explicit programmed. A computer program is said to learn from experience E w.r.t some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.
\nline
\uline{Why analyze data?}
\begin{itemize}
	\item Often information is hidden in the data but not readily evident
	\item Human analysts may take weeks to discover useful information
	\item Lots of commercial data is being collected and warehouse
	\item Computers have become cheaper and more powerful
	\item Data is collected and stored at enormous speeds (GB/hour)
	\item Data mining may help scientists in classifying and segmenting data
\end{itemize}
\subsection{Data Exploration}
\textbf{Data Exploration is a preliminary exploration of the data to better understand its characteristics.}\\ The key motivations of data exploration include:
\begin{itemize}
	\item Helping to select right tool for preprocessing or analysis
	 \item Making use of humans abilities to recognize patterns
\end{itemize}
Basic Traditional techniques of data exploration:
\begin{itemize}
	\item \textbf{Summary statistics}: are numbers that summarize properties of the data, such as frequency (the frequency of an attribute value is the percentage of times the value occurs in the data set). Most summary statistics can be calculated in a single pass through the data.
	\begin{itemize}
		\item \uline{Mean}: the most common measure of the location of an ordered set of points. However, it is very sensitive to outliers.
		\item \uline{Median}: also commonly used (p-50 percentile)
		\item \uline{Range}: the difference between the max and the min
		\item \uline{Variance and Standard Deviation}: most common measures of the spread of a set of points. This is also sensitive to outliers.
		\item For continuous data, is useful to know the notion of \uline{\textit{percentile}}. \\ 
	\textit{Given a continuous attribute x and a number p between 0 and 100, the p-th percentile is a value $x_p$ of x such that p\% of the observed values of x are less than $x_p$} 
\end{itemize}	
	\item \textbf{Visualization}: is the conversion of data into a visual or tabular format so that the characteristics of the data and the relationships among data items or attributes can be analyzed or reported. 
	\begin{itemize}
			\item Human have a well developed ability to analyze large amounts of information that is presented visually
			\item Can detect general patterns and trends
			\item Can detect outliers and unusual patterns
	\end{itemize}
	\uline{Selection} is the elimination of certain objects and attributes. It may involve choosing a subset of attributes.
	\begin{itemize}
		\item \textit{Dimensionality Reduction} is often used to reduce the number of dimensions to two or three
	\end{itemize}
	Visualization Techniques:
	\begin{itemize}
		\item Histograms:
		\begin{itemize}
			\item usually shows the distribution of values of a single variable
			\item divide the values into bins and show a bar plot of the number of objects in each bin
			\item  the height of each bar indicates the number of objects
\end{itemize}		 
		\item Box Plots 
		\begin{itemize}
			\item displays the distribution of data (over percentiles)
			\item can be used to compare attributes
		\end{itemize}
	\end{itemize}
	\item \textbf{Online Analytical Processing} (OLAP)
\end{itemize}
\subsection{What is Data Mining}
\textbf{Data Mining} is the non-trivial extraction of implicit, previously unknown and potentially useful information from data. It is the automatic (or semi-automatic) exploration and analysis of a large quantities of data in order to discover meaningful patterns. \\
The Data Mining Tasks are:
\begin{itemize}
	\item \textbf{Predictive methods}: use some variables to predict unknown or future values of other variables. Related to \textit{supervised learning}, which consists in inferring a function from labeled training data. The most used method is \uline{classification}.
	\item \textbf{Descriptive methods}: find human-interpretable patterns that describe the data. Related to \textit{unsupervised learning}, where a machine learning algorithm is used to draw inferences from datasets consisting of input data without labeled responses. The most used method is \uline{cluster analysis}, often used for exploratory data analysis to find hidden patterns of grouping in data.
\end{itemize}
\subsubsection{Methods}
\begin{itemize}
	\item \textbf{Classification} (predictive): given a collection of records, each record contains a set of attributes, one of which is the class. The goal is to find a model for the "class" attribute as a function of the values of other attributes, in order to assign a class to a previously unseen record as accurately as possible. The accuracy of the model is then evaluated over a set of unseen records called test set. \\ (Examples: predict fraudolent cases in credit card transactions)
	\item \textbf{Clustering} (descriptive): tries to divide data points into cluster such that data points in one cluster are more similar to one another and data points in separate clusters are less similar to one another. \\ 
	(Examples: market segmentation into distinct subsets of customers, find group of documents that are similar to each other based on the important terms appearing in them).
	\item \textbf{Frequent Itemset}: 
	\begin{itemize}
		\item an \uline{itemset} is a collection of one or more items (e.g., {Milk, Bread, Diaper})
		\item the \uline{support count} is the number of occurrences of an item set in a list of transactions (e.g., 2)
		\item the \uline{support} is a fraction of transactions that contain an item set (e.g., 2/5)
		\item the \uline{frequent itemset} is an item whose support is greater than or equal to a minsup threshold.
	\end{itemize}	 
	\item \textbf{Association Rule} An implication expression of the form X $\rightarrow$ Y, where X and Y are itemsets.
	\item \textbf{Association Rule Discovery} (descriptive): given a set of records each of which contains some number of items from a given collection, produce dependency rules which will predict occurrence of an item based on occurrences of other item. (e.g., If a customer buys diaper and milk, then she is very likely to buy beer). (Examples: marketing and sales promotions).
	\item \textbf{Sequential pattern discovery} (descriptive): 
	\begin{itemize}
		\item a sequence is an ordered list of elements (transactions)
		\item each element contains a collection of events (items)
		\item Example:
		\begin{itemize}
			\item Sequence: purchase history of a given customer
			\item Element: a set of items bought by a customer at time t
			\item Event: Books, diary products, CDs, etc.
		\end{itemize}
		\item The support of a subsequence $w$ is defined as the \textit{fraction of data sequences} that contain $w$ at least once
		\item a \uline{sequential pattern} is a frequent subsequence \\(i.e., a subsequence whose support is $>= minsup$) 
		\item Sequential Pattern Mining:
		\begin{itemize}
			\item given a database of sequences and a user-specific minimum support threshold (minsup)
			\item find all subsequences with support $>= minsup$
		\end{itemize}
\end{itemize}		
	\item \textbf{Regression} (predictive): predict a value of a given continuous valued variable based on the values of other variables, assuming a linear or nonlinear model of dependency. \\ 
	(Examples: Predicting sales amounts of a new product based on advertising expenditure; predicting wind velocities as a function of temperature, humidity, etc.)
	\item \textbf{Anomaly/Outlier Detection} (predictive): 
	\begin{itemize}
		\item Anomalies/outliers are the set of data points that are considerably different from the remainder of the data. 
		\item  The first step is to build a profile of the normal behavior, then use the normal profile to detect anomalies.
		\item Applications: 
		\begin{itemize}
			\item credit card fraud detection
			\item fault detection
			\item network intrusion detection
		\end{itemize}
	\end{itemize}	
\end{itemize}
\subsubsection{Challenges of Data Mining}
\begin{itemize}
	\item Scalability
	\item Dimensionality (reduction)
	\item Complex and Heterogeneous Data
	\item Data Quality
	\item Data Distribution
	\item Privacy Detection
	\item Streaming Data
\end{itemize}

\section{Data Warehouse}
\subsection{What is a Data Warehouse}
\begin{itemize}
	\item \textbf{As a dataset}: decision support database maintained separately from the organization’s operational database. A Data Warehouse is a single, complete and constant store of data obtained from a variety of different sources made available to end users, so that they can understand and use it in a business context.
	\item \textbf{As a process}: technique for assembling data from various sources with the purpose of answering business questions. A Data Warehouse is a process for transforming data into information and for making it available to users in a timely enough manner to make a difference.
\end{itemize}
A data warehouse is a
\begin{itemize}
	\item subject-oriented,
	\item integrated,
	\item time-varying,
	\item non-volatile
\end{itemize} 
collection of data that is used primarily in \textbf{organizational decision making}. \\
Data Warehouses (DWs) are very large databases (from Terabytes: $10^{12}$ bytes, to Zottabytes: $10^{24}$ bytes). \nline
A DW is a data collection built to support decision-making processes. It must allow analytical queries useful for the management of the company. It is usually built starting from several data sources and we update it only periodically. It contains synthetic and aggregated data. In a dynamic environment, one must perform periodically, building up a history of the enterprise. The main purpose of a data warehouse is to allow systematic or ad-hoc data analysis and mining. \nline
DW is a specialized DB, with different kind of operations:
\begin{itemize}
	\item \uline{\textit{\textbf{Warehouse (OLAP)} $\leftrightarrow$ Standard Transactional DB (OLTP)}}
	\item \textbf{Mostly reads} $\leftrightarrow$ Mostly updates
	\item \textbf{Queries are long and complex} $\leftrightarrow$ Many small transactions
	\item \textbf{Lots of scan} $\leftrightarrow$ Index/hash on primary key
	\item \textbf{Summarized, reconciled data} $\leftrightarrow$ Raw data
	\item \textbf{Hundreds of users} $\leftrightarrow$ Thousands of users
\end{itemize}
Where is a DW useful?
\begin{itemize}
	\item Commerce: sales and complaints analysis, client fidelization, shipping and stock control
	\item Financial services: risk and credit card analysis, fraud detection
	\item Telecommunications: call flow analysis
	\item Healthcare structures: patients’ ingoing and outgoing flows, cost analysis.
\end{itemize}
A Data Warehouse is divided in Data Marts. Indeed a \textbf{Data Mart} is a structure used to retrieve data from the DW. It is a subset of the data warehouse and is usually oriented to a specific business area.
\subsubsection{Evolution of Data Warehouses}
\begin{itemize}
	\item \textbf{Offline DW}: periodically updated from data in the operational systems and the DW data are stored in a data structure designed to facilitate reporting
	\item \textbf{Online DW}: data in the warehouse is updated for every transactions performed on the source data (e.g. by triggers)
	\item \textbf{Integrated DW}: data assembled from different data sources, so users can look up the information they need across other systems.
\end{itemize}
\subsection{Data Model for OLAP}
The most appropriate data model is the \textbf{data cube}, where the dimensions are the search keys and cube cells contain metric values (measures of the business). The data cube is a multidimensional array of values, used to represent data (facts) based on some measure of interest. It is useful to analyze data from different perspectives. \nline
The dimensional fact model allows one to describe a set of fact schemata. \\
The components of a fact schema are:
\begin{itemize}
	\item \textbf{Facts}: 
	\begin{itemize}
		\item a fact is a concept that is relevant for the decisional process
		\item typically it models a set of events of the organization
	\end{itemize}
	\item \textbf{Measures}: a measure is a numerical property of a fact
	\item \textbf{Dimensions}:
	\begin{itemize}
		\item a dimension is a fact property defined w.r.t a finite domain
		\item it describes an analysis coordinate for the fact.
	\end{itemize}
	\item \textbf{Dimension Hierarchy}: is a directional tree whose
	\begin{itemize}
		\item nodes are dimensional attributes
		\item edges describe n:1 associations between pairs of dimensional attributes
		\item root is the considered dimension
\end{itemize}
	\item \uline{Example}:
	\begin{itemize}
		\item Store chain:
		\begin{itemize}
			\item Fact: sales
			\item Measures: sold quantity, gross income
			\item Dimensions: product, time, zone
		\end{itemize}
	\end{itemize}
\end{itemize}
\subsubsection{OLAP Operations}
\begin{itemize}
	\item \textbf{Roll-up}: aggregates data at a higher level (e.g., last year's sales volume per product category and per region)
	\item \textbf{Drill-down}: de-aggregates data at the lower level (e.g., for a given product category and a given region, show daily sales)
	\item \textbf{Slice-and-dice}: applies selections and projections, which reduce data dimensionality
	\item \textbf{Pivoting}: selects two dimensions to re-aggregate data
	\item \textbf{Ranking}: sorts data according to predefined criteria
	\item \textbf{Traditional operations} (select, project, join, derived attributes, etc)
\end{itemize}
\begin{figure}[htp]
\centering
\includegraphics[width=.3\textwidth]{images/olap1}\hfill
\includegraphics[width=.3\textwidth]{images/olap2}\hfill
\includegraphics[width=.3\textwidth]{images/olap3}\hfill
\end{figure}
\pagebreak
\subsubsection{OLAP LOGICAL MODELS}
\begin{itemize}
	\item \textbf{MOLAP} (Multidimensional On-Line Analytical Processing): 
	\begin{itemize}
		\item stores data by using a multidimensional data structure
		\item the storage is not in the relational database, but in proprietary formats
		\item \uline{Advantages}:
		\begin{itemize}
			\item Excellent performance, fast data retrieval, optimal for slicing and dicing operations
			\item Can perform complex calculations (pre-generated when the cube is created)
		\end{itemize}
		\item \uline{Disadvantages}:
		\begin{itemize}
			\item Limited in the amount of data it can handle (calculations are performed when the cube is built)
			\item Requires additional investment: cube technology are often proprietary and do not already exist in the organization (human and capital resources are needed)
		\end{itemize}
	\end{itemize}		
	\item \textbf{ROLAP} (Relational On-Line Analytical Processing):
	\begin{itemize}
		\item Uses the relational data model to represent multidimensional data.
		\item \uline{Advantages}:
		\begin{itemize}
			\item Can handle large amount of data
			\item Can leverage functionalities inherent in the relational databases
		\end{itemize}
		\item \uline{Disadvantages}:
		\begin{itemize}
			\item Performance can be slow because ROLAP report is essentially a SQL query (or multiple queries) in the relational database (query time is long if data size is large)
			\item Limited by SQL functionalities
		\end{itemize}
	\end{itemize}		
	\item \textbf{HOLAP} (Hybrid On-Line Analytical Processing):
	\begin{itemize}
		\item combines the advantages of MOLAP and ROLAP.
		\item for summary-type information it leverages cube technology for faster performance
		\item when detail information is needed it can "drill through" from the cube into the underlying relational data
\end{itemize}	 
\end{itemize}
\myparagraph{The Data Cube in SQL}
\begin{itemize}
	\item \textbf{WITH CUBE}: 
	\begin{itemize}
		\item generates a result set that shows aggregates for all combinations of values in the selected columns
		\item evaluates aggregate expression \uline{with all possible combinations of columns specified in group by clause}
	\end{itemize}
	\item \textbf{WITH ROLLUP}:
	\begin{itemize}
		\item generates a result set that shows aggregates for a hierarchy of values in the selected columns
		\item evaluates aggregate expressions \uline{only relative to the order of columns specified in group by clause}
		\item it eliminates the results that contain ALL only in one column
	\end{itemize}		
\end{itemize}
\subsection{Data Warehouse Design}
A \textbf{primary event} is an occurrence of a fact. It is represented by means of tuple of values (e.g., On 10/10/2001, ten "Brillo" detergent packets were sold at the BigShop for a total amount of 25 euros). \\
A \textbf{hierarchy} describes how it is possible to group and select primary events. The root of a hierarchy corresponds to the primary event, and represents the \textit{finest aggregation granularity}.
Given a set of dimensional attributes, each tuple of their values identifies a \textbf{secondary event} that aggregates (all) the corresponding primary events. \\ For example the sales can be grouped by Product and Month: "in October 2001, 230 "Brillo" detergent packets were sold at the BigShop for a total amount of 575 euros". \nline
A \textbf{descriptive attribute} contains additional information about a dimensional attribute. They are uniquely determined by the corresponding dimensional attribute (1:1 relation). \\
A \textbf{cross-dimensional attribute} is a dimensional or a descriptive attribute whose value is obtained by combining values of some dimensional attributes. \\
In a fact schema, some portions of a hierarchy might be duplicated. As a shorthand we allow \textbf{hierarchy sharing}. If the sharing starts witha dimension attribute, it is necessary to indicate the roles on the incoming edge. \\
\textbf{Aggregation requires to specify an operator} to combine values related to primary events into a unique value related to a secondary event. (e.g., sum of sold quantity aggregated by month). \\
A measure is \textbf{additive w.r.t a given dimension} iff the SUM operator is freely applicable to that measure along that dimension without the need of any additional information (e.g. amount sold is additive w.r.t. time and space while total number of products sold is non-additive w.r.t. to time). \nline
It is possible to identify three different measure categories:
\begin{itemize}
	\item \textbf{Flow measures}: related to a time period 
	\begin{itemize}
		\item e.g., N. of sales per day, N. of births in a year.
		\item  At the end of the period the measures are evaluated in a cumulative way (SUM, AVG, MIN, MAX).
	\end{itemize}
	\item \textbf{Level measures}: evaluated in particular time instants
	\begin{itemize}
		\item e.g., N. products in stock, N. inhabitants in a city
		\item SUM only on non temporal hierarchies, AVG, MIN, MAX.
		\item N. products in stock can be aggregated by SUM over the category/type or the shop/city hierarchies, but NOT over time hierarchies
\end{itemize}
	\item \textbf{Unitary measures}: relative measures 
	\begin{itemize}
		\item e.g., unitary price at a given instant, money change rate, interest rate
		\item They are evaluated in particular time instants but they are relative measures  (AVG, MIN, MAX, but relative measures)
		\item unitary price at a given instant cannot be aggregated by sum over the category/type or the shop/city, nor over the time hierarchy
	\end{itemize}		
\end{itemize}
\begin{figure}[htp]
\center
\includegraphics[width=150pt]{images/measures-categories}\hfill
\end{figure}
\subsubsection{Aggregate Operators}
\begin{itemize}
	\item \textbf{Distributive operator}: allows to aggregate data starting from partially aggregated data (e.g., sum, max, min)
	\item \textbf{Algebraic operator}: requires further information to aggregate data (e.g., avg)
	\item \textbf{Holistic operator}: it is not possible to obtain aggregate data starting from partially aggrega data (e.g., mode, median)
\end{itemize}
\subsubsection{Star Schema vs. Snowflake Schema}
\myparagraph{STAR SCHEMA}
A \textbf{Star Schema} is:
\begin{itemize}
	\item A set of relations $DT_1, DT_2,..., DT_n$ (\textbf{dimension tables}) each corresponding to a dimension
	\begin{itemize}
		\item Dimension tables are de-normalized, and this introduces redundancy, but fewer joins to do
	\end{itemize}
	\item Each $DT_i$ is characterized by a primary key $d_i$ and by a set of attributes describing the analysis dimensions with different aggregation levels
	\item A relation $FT$, \textbf{fact table}, that imports the primary keys of dimensions tables. The primary key of $FT$ is $d_1,d_2,d_n$. FT contains also an attribute for each measure.
\end{itemize}
\myparagraph{SNOWFLAKE SCHEMA}
The \textbf{Snowflake Schema} reduces the de-normalization of the dimensional tables $DT_i$ of a Star Schema (removal of some transitive dependencies). This allows to avoid space wasting. \\
Dimension tables of a Snowflake schema are composed by:
\begin{itemize}
	\item A primary key $d_{i,j}$
	\item A subset of $DT_i$ attributes that directly depend on $d_{i,j}$
	\item Zero or more external keys that allow to obtain the entire information
\end{itemize}
In a Snowflake Schema:
\begin{itemize}
	\item \textbf{Primary dimension tables}: their keys are imported in the fact table
	\item \textbf{Secondary dimension table}
\end{itemize}
Benefits:
\begin{itemize}
	\item Reduction of memory space
	\item New surrogate keys
	\item Advantages in the execution of queries related to attributes contained into fact and primary dimension tables
\end{itemize}
\begin{figure}[htp]
\centering
\includegraphics[width=.4\textwidth]{images/star}\hfill
\includegraphics[width=.4\textwidth]{images/snowflake}\hfill
\end{figure}
\subsubsection{Conceptual Design}
Conceptual design takes into account the documentation related to the reconciled database.
\begin{itemize}
	\item Conceptual Schema (E/R, UML class diagram, ...)
	\item Logical Schema (e.g., relational, XML, ...)
\end{itemize}
Top-down methodology:
\begin{enumerate}
	\item \textbf{Fact definition}: facts correspond to events that dynamically happen in the organization 
	\item For each fact:
	\begin{enumerate}
		\item \textbf{Design of the attribute tree}
		\item \textbf{Attribute tree editing}: allows to remove irrelevant attributes
		\begin{itemize}
			\item \textit{pruning}: the subtree rooted in v is deleted. Delete a leaf node or a leaf subtree.
			\item \textit{grafting}: the children of v are directly connected to the father of v. Delete an internal node, and attach its children to the parent of the internal node.
		\end{itemize}
		\item \textbf{Dimension definition}: dimensions can be chosen among the attributes that are children of the root of the tree (time should always be a dimension)
		\item \textbf{Measure definition}: if the fact identifier is included in the set of dimensions, then numerical attributes that are children of the root (fact) are measures. More measures are defined by applying aggregate functions to numerical attributes of the tree.
		\item \textbf{Fact schema creation}: the attribute tree is translated into a fact schema including dimensions and measures. The fact name corresponds to the name of the selected entity, dimension hierarchies correspond to subtrees having as roots the different dimensions (with the least granularity)
	\end{enumerate}
\end{enumerate}
In the \textbf{glossary}, an expression is associated with each measure. The expression describes how we obtain the measure at the different levels of aggregation starting from the attributes of the source schema. 
\pagebreak
\section{Big Data Architectures and Data Ethics}
\subsection{NoSQL Databases}
\subsubsection{Transactional Systems}
ACID properties are a consistency model over data in transactions, that is common in traditional relational databases. They guarantee safe operations on data at anytime. \\
The ACID acronym stands for:
\begin{itemize}
	\item \textbf{Atomicity}: a transaction is an indivisible unit of execution
	\item \textbf{Consistency}: the execution of a transaction must not violate the integrity constraints
defined on the database
	\item \textbf{Isolation}: the execution of a transaction is not affected by the execution of other
concurrent transactions
	\item \textbf{Durability} (Persistence): the effects of a successful transaction must be permanent.
\end{itemize}
The classical DBMSs (also distributed) are \uline{transactional systems}: they provide a mechanism for the definition and execution of transactions. In the execution of a transaction the ACID properties must be guaranteed. A transaction represents the typical elementary unit of work of a Database Server, performed by an application. \\
Because ACID properties are not really required in certain domains,
\uline{new DBMS have been proposed that are not transactional systems}. 
\subsubsection{Big Data and the Cloud}
\textbf{Data Clouds}: \uline{on demand storage services}, reliable, offered on the internet with easy access to a virtually infinite number of storage resources, computing and network. \nline
\uline{Classification of possible methods of storage}:
\begin{itemize}
	\item \textbf{Centralized or distributed}: transactional, based on a traditional model (e.g., relational), most widely used for traditional business applications
	\item \textbf{Federated and multi-databases}: used for companies and organization that are associated and share their data on the Internet
	\item \textbf{Cloud databases}: to support Big Data by means of load sharing and data partitioning (usually NoSQL)
\end{itemize}
\subsubsection{NoSQL databases}
It has been realized that is not always necessary that a system for data management guarantees all transactional characteristics. The non-transactional DBMS are commonly called NoSQL DBMS. This is really not correct because the fact that a system is relational (and uses the SQL language) and that it has a transactional characteristics are independent. \\
NoSQL databases:
\begin{itemize}
	\item Provide \textbf{flexible schemas}
	\item \textbf{The updates are performed asynchronously} (no explicit support for concurrency)
	\item Potential inconsistencies in the data must be solved directly by users
	\item \textbf{Scalability}: no joins, no 2PhaseCommit
	\item Object-oriented friendly
	\item Caching easier
	\item Easily evolved to live replicas, made possible by the simplicity of re-partitioning of data
	\item \textbf{Do not support all the ACID properties}
\end{itemize}
\pagebreak
\subsubsection{NoSQL for Data Warehouses?}
\begin{figure}[htp]
\centering
\includegraphics[width=300pt]{images/olap-vs-oltp}\hfill
\caption{OLAP is typical of Data Warehouses}
\end{figure}
NoSQL main strengths are \textbf{scalability} and \textbf{flexibility}.
\begin{itemize}
	\item \uline{Advantages}:
	\begin{itemize}
		\item No updates in DWs $\rightarrow$ no need of ACID properties
		\item NoSQL databases provide very high read (and write) throughput on large objects
	\end{itemize}
	\item \uline{Disadvantages}:
	\begin{itemize}
		\item NoSQL databases aren't intended for use in atomic level data
		\item they also don't provide any kind of additional basic data transformation within the database
	\end{itemize}
\end{itemize}
\subsubsection{Data Model}
\begin{itemize}
	\item \textbf{Key-Value}
	\begin{itemize}
		\item single key
		\item Value: opaque
		\item Querying: find by key
		\item No schema
		\item Standard APIs: get/put/delete
		\item No relationships in the database $\rightarrow$ easier to scale!
		\item Decoupled and denormalized entities are "self-contained"
		\item Sharding (horizontal partitioning)
	\end{itemize}
	\item \textbf{Document-based}
	\begin{itemize}
		\item a unique key represent a document
		\item Value: collection of documents whose structure is not fixed
		\item Ability to query: very rich (filter on the internal fields of the document)
		\item Free and less sensitive to design than a column-based
	\end{itemize}
	\item \textbf{Column-family data model}
	\begin{itemize}
		\item key is a triple row/column/timestamp
		\item Value “opaque”
		\item Strongly oriented to BigData
		\begin{itemize}
			\item maximum scalability
			\item data is partitioned horizontally and vertically based on the keys of the row and column (sharding)
		\end{itemize}		 
		\item Ability to query: get or filter only on row and column keys
		\item Semi-structured diagram:
		\begin{itemize}
			\item Row: columns indexed within each row by a row-key
			\item Column-family: a set of columns, normally similar in structure to optimize compaction
			\item Columns in the same column family will be "close" (stored in the same bloc on disk)
			\item Columns: have a name and may contain a value for each row
		\end{itemize}
	\end{itemize}
	\item \textbf{Graph-based}
	 \begin{itemize}
	 	\item data relations are well represented as a graph consisting of elements interconnected with a finite number of relations between them.
	 \end{itemize}
\end{itemize}
\myparagraph{Famous Implementations}
\begin{itemize}
	\item \textbf{Amazon DynamoDB}:
	\begin{itemize}
		\item Key-value
		\item CAP: AP, guarantees Availability and Partition tolerance, relaxing consistency
	\end{itemize}
	\item \textbf{Google BigTable}:
	\begin{itemize}
		\item Column-oriented
		\item CAP: CP, if there is a network partition, Availability is lost, but strict consistency may be required
	\end{itemize}
	\item \textbf{Cassandra}:
	\begin{itemize}
		\item Column-oriented
		\item CAP: AP, consistency is configurable
	\end{itemize}
	\item \textbf{MongoDB}:
	\begin{itemize}
		\item Document-based
		\item CAP: CP
	\end{itemize}
\end{itemize}
\subsubsection{CAP Theorem}
A data management system shared over the network can guarantee at most two of the following properties:
\begin{itemize}
	\item \textbf{Consistency (C)}: all nodes see the same data at the same time
	\item \textbf{Availability (A)}: every request receives a response on success or failure (regardless of the
state of any individual node in the system)
	\item \textbf{Partition Tolerance (P)}: the system continues to operate despite the number of message about loss or failure of part of the system (i.e., network partitions)
\end{itemize}
Note that all three properties are more "fuzzy" than binary. In particular there's a trade-off between Consistency and Availability. Increasing the support for one of them, will result in decreasing the support for the other. \\
N.B. In ACID the C means that a transaction preserves all the database constraints; in CAP the C refers only to copy consistency, a strict subset of ACID consistency.
\pagebreak
\subsection{Data Ethics}
As data have an impact on almost every aspect of our lives, it is more and more important to understand the nature of this effect. With search and recommendation engines, the web can influences our lives. \\
Big Data processing is based on algorithmic, thus it must be objective. Unfortunately:
\begin{itemize}
	\item algorithms are based on data, and data may contain errors
	\item often the algorithm produce opaque processes that cannot be explained
	\item technology must be accompanied by ethical and legal considerations
\end{itemize}
It is up to the data scientists to:
\begin{itemize}
	\item identify which datasets can genuinely help answering some given question
	\item understand their contents
	\item choose the most appropriate knowledge extraction technique (search, query, or data analysis methods) to obtain a fair result
\end{itemize}
This sequence of choices may strongly influence the process, and biased results might be obtained.
\subsubsection{Ethical Dimensions}
\begin{itemize}
	\item \textbf{Data protection}: of human rights and their consequences in normative ethics.
	\item \textbf{Fairness}: fairness of data is defined as the absence of bias.
	\item \textbf{Transparency}: is the ability to interpret the information extraction process in order to
verify which aspects of data determine its results. In this context, transparency metrics can use the notions of:
\begin{itemize}
	\item \textit{data provenance}: describing where the original data come from
	\item \textit{explanation}: describing how a result has been obtained
\end{itemize}
	\uline{Transparency may conflict with Data Protection.}
	\item \textbf{Diversity}: is the degree to which different kinds of objects are represented in a dataset. \uline{The diversity dimension may conflict with data quality needs}, that prioritize the use of few, high-reputation sources.
\end{itemize}
\textbf{Data quality} is a typical ethical requirement: we could never trust a piece of information if it did not have the typical data quality properties. \\ Data should conform to a high ethical standard, for it to be considered of good quality. \\
Hence, the satisfaction of the ethical requirements is actually necessary to assert the quality of a result. It is the responsibility of the system designer and of the person/company that ordered the job, to ensure that the necessary ethical properties are satisfied.
\pagebreak
\section{Data Quality}
\subsection{Introduction}
Data preparation is important because real-world data is often incomplete, inconsistent, and contain many errors. Data preparation, cleaning, and transformation comprises the majority of the work in a data mining application (90\%).
\begin{figure}[htp]
\centering
\includegraphics[width=300pt]{images/data-quality-integration}\hfill
\end{figure} \\
\textbf{Data Quality} is the ability of a data collection to meet user requirements. 
Causes of poor quality:
\begin{itemize}
	\item \textit{Historical changes}: the importance of data might change over time
	\item \textit{Data usage}: data relevance depends on the process in which data are used
	\item \textit{Corporate mergers}: data integration might cause some difficulties
	\item \textit{Privacy}: data are protected by privacy rules and thus it is difficult to find data to correct and its own db
	\item \textit{Data enrichment}: it might be dangerous to enrich internal data with external sources.
\end{itemize}
Poor data quality effects:
\begin{itemize}
	\item the customer satisfaction decreases
	\item poor DQ is expensive
	\item poor DQ impacts on decisional processes
	\item poor DQ affects long-term strategies
\end{itemize}
\subsection{Data Quality Management}
Data quality management is performed in four phases:
\begin{enumerate}
	\item Quality dimensions definition
	\item Quality dimensions assessment
	\item Quality issues analysis
	\item Quality improvement
\end{enumerate}
\subsubsection{Quality Dimensions}
The most used objective dimensions are:
\begin{itemize}
	\item \textbf{accuracy}: the extent to which data are correct, reliable and certified.
	\item \textbf{completeness}: the degree to which a given data collection includes the data describing the corresponding set of real-world objects.
	\item \textbf{consistency}: the satisfaction of semantic rules defined over a set of data items. It refers to the violation of semantic rules defined over a set of data items.
	\item \textbf{timeliness}: the extent to which data are sufficiently updated for a task. It is the average age of the data in a source.
\end{itemize}
\subsubsection{Assessment Techniques}
\begin{itemize}
	\item Objective measuring: 
	\begin{itemize}
		\item completeness $\rightarrow$ absence of null values
		\item accuracy $\rightarrow$ distance between the current value and the correct value
		\item consistency $\rightarrow$ ratio of correct values w.r.t. integrity and business rules
		\item timeliness $\rightarrow$ max(0; 1 - currency/violatility)
\end{itemize}		
	\item Questionnaires
	\begin{itemize}
		\item definition of 12-20 items for data quality dimensions
		\item Item: "this information is (attribute or phrase)"
		\item e.g., this information is presented consistently, this information is relevant to our work
	\end{itemize}
\end{itemize}
\textbf{Data Quality Rules} are the requirements that business set to their data and they are associated with the data quality dimensions. They are also designated to check the validity of data.
\subsubsection{Analysis Techniques}
\begin{itemize}
	\item Benchmarking Gap Analysis
	\item Role Gap Analysis
	\item Plotting Data
\end{itemize}
\subsubsection{Data Quality Improvement}
\begin{itemize}
	\item Data-oriented improvement methods (e.g. Data Cleaning)
	\item Process-oriented improvements methods
\end{itemize}
\myparagraph{Data Cleaning}
\textbf{Data Cleaning} is the process of identifying and eliminating inconsistencies, discrepancies and errors in data in order to improve quality. \\
Cleaning tasks
\begin{itemize}
	\item \textbf{Standardization/normalization}:
	\begin{itemize}
		\item Datatype conversion
		\item Discretization
		\item Domain Specific
	\end{itemize}
	\item \textbf{Missing Values}:
	\begin{itemize}
		\item Detection
		\item Imputing
	\end{itemize}
	\item \textbf{Outlier Detection}:
	\begin{itemize}
		\item Model
		\item Distance
	\end{itemize}
	\item \textbf{Duplicate detection}:
	\begin{itemize}
		\item discovery of multiple representations of the same real-world object and merging.
	\end{itemize}	
\end{itemize}
\pagebreak
\myparagraph{Duplicate Detection Issues}
\begin{itemize}
	\item Representations are not identical $\rightarrow$ similarity measures
	\item Datasets are large $\rightarrow$ algorithms
\end{itemize}
Similarity measures:
\begin{itemize}
	\item String-Based Distance Functions
	\begin{itemize}
		\item Edit-distance (minimum number of edits from one word to the other)
		\item Soundex (phonetical similarity measure)
	\end{itemize}
	\item Item-Based Distance Functions
	\begin{itemize}
		\item Jaccard distances (intersection of two sets of words divided by the union of them)
		\item Cosine similarity
	\end{itemize}
\end{itemize}
In order to check the existence of duplicates we should compare all pairs of instances. \textbf{Too many comparisons} $\rightarrow$ ($n^2-n)/2$. \\
\textbf{Partitioning} is the solution for this problem. Partition records through a selection and comparisons are performed among pairs of records inside the partition. \\ 
Example: \uline{Sorted neighborhood}
\begin{enumerate}
	\item Creation of key: compute a key for each record by extracting relevant fields
	\item Sort data: using the key 
	\item Merge data: move a fixed size window through the sequential list of records. This limits the comparisons to the records in the window. 
	\item Data are compared within a rule and a similarity function
\end{enumerate}
\subsubsection{Data Fusion}
The data integration process is composed of three steps:
\begin{enumerate}
	\item Schema alignment
	\item Entity reconciliation
	\item Data fusion
\end{enumerate}
\textbf{Data Fusion} resolves uncertainties and contradictions. Given duplicate records (previously identified), it creates a single object representation while resolving conflicting data values.
\begin{itemize}
	\item complementary tuples
	\item identical tuples
	\item subsumed tuples
	\item conflicting tuples
\end{itemize}
\myparagraph{Data Fusion ANSWERS}
The result of a query to an integrated information system is called "\textbf{answer}". \\ 
Properties of an answer:
\begin{itemize}
	\item \textbf{Complete}: the answer should contain all the objects and attributes that have been present in the sources
	\item \textbf{Concise}: all the object and attributes are described only once
	\item \textbf{Consistent}: all the tuples that are consistent w.r.t. a specified set of integrity constraints are present
	\item \textbf{Complete and consistent}: it additionally fulfill a key constraint on some real world ID (contains all attributes from the sources and combines semantically equivalent ones into only one attribute)
\end{itemize}
- 
\end{document}
